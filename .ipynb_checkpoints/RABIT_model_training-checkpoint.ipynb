{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2292ce28-0d42-4758-8f60-82daa2347249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr, zscore\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import XGBRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2d9032-a87a-4a2f-8cb5-740255321af3",
   "metadata": {},
   "source": [
    "# RABIT model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20146230-8b94-435e-8950-e65a3ad58cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV file into a DataFrame\n",
    "df_reprs_final= pd.read_csv('/path/to/motor_representations/ehr_representations_random.csv')\n",
    "## get protein expression for selected protein\n",
    "prot = pd.read_csv('/path/to/measured_proteomics/from/uk_biobank/measured_proteomics_random.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b26b59-258a-4796-9fcf-5e2e38dccfd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list of protein names\n",
    "poi_list = [col[:-8] for col in prot.columns if col.endswith('_protein')]\n",
    "poi_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109bbb99-3e19-4ca8-8d12-2c7df855a25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='/path/to/outputdir/mylog.log', mode='w')  # 'a' means append mode\n",
    "formatter = logging.Formatter('%(asctime)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afee224-6afa-47e4-ac14-c6945bdbb65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for collection of accuracy metrics\n",
    "data = {\"protein\": [], 'pearson_correlation': [], 'pearson_pvalue':[], \"spearman_correlation\": [], \"spearman_pvalue\": [], \"best_train_rmse\": [],  \"best_val_rmse\": [], \"best_test_rmse\": []}\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56e592-357e-4157-9c5e-ba4fd44e8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to collect predictions\n",
    "predvalues_df = pd.DataFrame({'sample_eid': prot['eid']})\n",
    "\n",
    "# function to populate the dataframe with new columns\n",
    "def populate_dataframe(df, poi, predictions, y):\n",
    "    column_name = poi + \"_protein\"\n",
    "    y.drop(columns=[column_name], inplace=True)\n",
    "    y[column_name] = predictions\n",
    "    df = pd.merge(df, y, left_on='sample_eid', right_on='patient_ids', how='left')\n",
    "    df = df.drop(columns=['patient_ids'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978e5199-dfb1-4cee-b56a-62384b15d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_org(protein_name, latentdf, prot):\n",
    "\n",
    "    # protein of interest\n",
    "    poi = protein_name #change this to select different proteins\n",
    "    poiname = poi + '_protein'\n",
    "    print(poiname)\n",
    "    # subset protein expression for poi\n",
    "    poidf = prot[[poiname, 'eid']]\n",
    "    # drop rows (patients) that don't have value for given protein\n",
    "    poidf = poidf.dropna()\n",
    "    # merge dfs and clean for xgboost by removing sample_ID and extraneous columns\n",
    "    merged_df = pd.merge(latentdf, poidf, left_on = 'patient_ids', right_on='eid', how='inner')\n",
    "    cols_remove = ['eid', 'labeling_time']\n",
    "    merged_clean = merged_df.drop(columns=cols_remove)\n",
    "\n",
    "    X, y = merged_clean.drop(poiname, axis=1), merged_clean[[poiname, 'patient_ids']]\n",
    "    return poi, X, y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def bootstrap_nn(poi, X, y, bootstrap, output_dir):    \n",
    "    # Create data structures below for bootstrapping\n",
    "    train_error_dict = {}\n",
    "    val_error_dict = {}\n",
    "    test_error_dict = {}\n",
    "    prediction_dict = {str(i): None for i in range(y.shape[0])}\n",
    "    featimp_dict = {col: [] for col in df_reprs_final.columns if col.startswith(\"data_\")}\n",
    "    poiname = poi + '_protein'\n",
    "    \n",
    "    unique_patient_ids = X['patient_ids'].unique()  # for bypatient split\n",
    "    unique_patient_ids_df = pd.DataFrame(unique_patient_ids, columns=['patient_ids'])  # for bypatient split\n",
    "    \n",
    "    # Create the models subdirectory inside the output_dir\n",
    "    models_dir = os.path.join(output_dir, \"models\")\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    \n",
    "    # Create the poi_model_weights directory inside the models directory\n",
    "    model_weights_dir = os.path.join(models_dir, f\"{poi}_model_weights\")\n",
    "    os.makedirs(model_weights_dir, exist_ok=True)\n",
    "    \n",
    "    for i in range(bootstrap):\n",
    "        \n",
    "        # ('patient_ids') is not a feature\n",
    "        input_size = X.shape[1] - 1\n",
    "        train_ids_initial, test_ids = train_test_split(unique_patient_ids_df, test_size=0.5, random_state=i)\n",
    "        \n",
    "        # split data into train and test (50/50 split)\n",
    "        cols_remove = ['patient_ids']\n",
    "        X_train_initial = X[X['patient_ids'].isin(train_ids_initial['patient_ids'])]\n",
    "        X_test = X[X['patient_ids'].isin(test_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        y_train_initial = y[y['patient_ids'].isin(train_ids_initial['patient_ids'])]\n",
    "        y_test = y[y['patient_ids'].isin(test_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        \n",
    "        # second split of training data into training and validation sets (80/20 split)\n",
    "        train_ids, val_ids = train_test_split(train_ids_initial, test_size=0.2, random_state=i)\n",
    "        \n",
    "        X_train = X_train_initial[X_train_initial['patient_ids'].isin(train_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        X_val = X_train_initial[X_train_initial['patient_ids'].isin(val_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        y_train = y_train_initial[y_train_initial['patient_ids'].isin(train_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        y_val = y_train_initial[y_train_initial['patient_ids'].isin(val_ids['patient_ids'])].drop(columns=cols_remove)\n",
    "        \n",
    "        # convert to PyTorch tensors\n",
    "        X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "        X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "        y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "        X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "        y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "        \n",
    "        # create datasets and dataloaders\n",
    "        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # build Model\n",
    "        class RegressionModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super(RegressionModel, self).__init__()\n",
    "                self.layer1 = nn.Linear(input_size, 32)  # Input to hidden layer\n",
    "                self.relu = nn.ReLU()\n",
    "                self.layer2 = nn.Linear(32, 1)  # Hidden to output layer\n",
    "        \n",
    "            def forward(self, x):\n",
    "                x = self.relu(self.layer1(x))\n",
    "                x = self.layer2(x)\n",
    "                return x\n",
    "        \n",
    "        model = RegressionModel()\n",
    "        \n",
    "        # criterion and optimizer\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        \n",
    "        # training loop with early stopping\n",
    "        num_epochs = 500  # Maximum number of epochs\n",
    "        patience = 10  # Number of epochs to wait\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        best_model_state = None\n",
    "        best_epoch = 0\n",
    "        epochs_no_improve = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "            train_loss = train_loss / len(train_loader.dataset)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item() * inputs.size(0)\n",
    "            val_loss = val_loss / len(val_loader.dataset)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # update best model if current validation loss is lower\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_model_state = model.state_dict()\n",
    "                best_epoch = epoch + 1\n",
    "                epochs_no_improve = 0  # Reset the counter if there is an improvement\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "            \n",
    "            # early stopping condition\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break\n",
    "            \n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # store training and validation losses for plotting\n",
    "        train_error_dict[i] = train_losses\n",
    "        val_error_dict[i] = val_losses\n",
    "        \n",
    "        # print best epoch\n",
    "        print(f'The best model was from epoch {best_epoch} with Validation Loss: {best_val_loss:.4f}')\n",
    "        \n",
    "        # load best model\n",
    "        model.load_state_dict(best_model_state)\n",
    "        \n",
    "        # save the model for this bootstrap iteration in the poi_model_weights directory\n",
    "        model_path = os.path.join(model_weights_dir, f'bootstrap_model_{i}.pt')\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        print(f\"Saved model for bootstrap iteration {i} at {model_path}\")\n",
    "        \n",
    "        # collect actual and predicted values for the best model\n",
    "        actual = []\n",
    "        predicted = []\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                actual.extend(labels.numpy().flatten())\n",
    "                predicted.extend(outputs.numpy().flatten())\n",
    "        \n",
    "        # collect predictions\n",
    "        for p in range(y_test.shape[0]):\n",
    "            key_to_append = str(y_test.index[p])\n",
    "            value_to_append = predicted[p]\n",
    "            if key_to_append in prediction_dict:\n",
    "                if prediction_dict[key_to_append] is None:\n",
    "                    prediction_dict[key_to_append] = []\n",
    "                prediction_dict[key_to_append].append(value_to_append)\n",
    "\n",
    "    return prediction_dict, train_error_dict, val_error_dict\n",
    "\n",
    "\n",
    "def plot_correlation(poi, y, prediction_dict, savepath):\n",
    "    from scipy.stats import pearsonr, spearmanr\n",
    "    # calculate average prediction for each sample\n",
    "    y_avg_pred = [sum(values) / len(values) if values is not None else None for values in prediction_dict.values()]\n",
    "    \n",
    "    # Convert y_test DataFrame to a numpy array and reshape\n",
    "    cols_remove = ['patient_ids']\n",
    "    y = y.drop(columns=cols_remove)\n",
    "    \n",
    "    y_np = y.to_numpy().reshape(-1)\n",
    "\n",
    "    # create a mask for numeric values in y_avg_pred\n",
    "    is_numeric_mask = np.array([isinstance(x, (int, float)) for x in y_avg_pred])\n",
    "\n",
    "    # apply the mask to both y_np and y_avg_pred to filter out rows where y_avg_pred is not numeric\n",
    "    filtered_y_np = y_np[is_numeric_mask]\n",
    "    filtered_y_avg_pred = np.array(y_avg_pred)[is_numeric_mask]\n",
    "\n",
    "    # calculate correlations using the filtered data\n",
    "    if len(filtered_y_np) > 1 and len(filtered_y_avg_pred) > 1: \n",
    "        pcorrelation, ppvalue = pearsonr(filtered_y_np, filtered_y_avg_pred)\n",
    "        scorrelation, spvalue = spearmanr(filtered_y_np, filtered_y_avg_pred)\n",
    "    else:\n",
    "        pcorrelation, ppvalue = np.nan, np.nan \n",
    "        scorrelation, spvalue = np.nan, np.nan  \n",
    "\n",
    "    # setting same axis range for both x and y axis\n",
    "    min_val = min(min(filtered_y_np), min(filtered_y_avg_pred))\n",
    "    max_val = max(max(filtered_y_np), max(filtered_y_avg_pred))\n",
    "\n",
    "     # plot predicted vs. actual values\n",
    "    plt.scatter(filtered_y_np, filtered_y_avg_pred, alpha=0.5)\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Average Predicted Values\")\n",
    "    plt.title(f\"{poi} Predicted vs. Actual\\n(Spearman Correlation: {scorrelation:.2f}, Pvalue: {spvalue})\")\n",
    "\n",
    "    \n",
    "    plt.xlim(min_val, max_val)\n",
    "    plt.ylim(min_val, max_val)\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], color='red', linestyle='--')\n",
    "    save_directory = savepath+\"/correlation/\"\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    modpoi = poi.replace(\"/\", \"_\")\n",
    "    plot_filename = os.path.join(save_directory, f\"{modpoi}_scorr.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.clf()\n",
    "    return poi, pcorrelation, ppvalue, scorrelation, spvalue, y_avg_pred\n",
    "\n",
    "def pad_errors(error_dict):\n",
    "    max_length = max(len(lst) for lst in error_dict.values())\n",
    "    padded_errors = {key: lst + [np.nan] * (max_length - len(lst)) for key, lst in error_dict.items()}\n",
    "    return padded_errors\n",
    "\n",
    "def plot_learning_curve(poi, train_error_dict, val_error_dict, savepath):\n",
    "    padded_train_errors = pad_errors(train_error_dict)\n",
    "    padded_val_errors = pad_errors(val_error_dict)\n",
    "    train_errors = np.array(list(padded_train_errors.values()))\n",
    "    val_errors = np.array(list(padded_val_errors.values()))\n",
    "\n",
    "    # calculate the mean error per epoch across all bootstrap iterations\n",
    "    mean_train_errors = np.nanmean(train_errors, axis=0)\n",
    "    mean_val_errors = np.nanmean(val_errors, axis=0)\n",
    "    epochs = np.arange(1, len(mean_train_errors) + 1)\n",
    "    \n",
    "   # plot the learning curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(epochs, mean_train_errors, label='Average Train Error', marker='o')\n",
    "    plt.plot(epochs, mean_val_errors, label='Average Validation Error', marker='x')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(f'{poi} Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # save the plot\n",
    "    save_directory = os.path.join(savepath, \"learning_curves\")\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    plot_filename = os.path.join(save_directory, f\"{poi.replace('/', '_')}_learning_curve.png\")\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.clf()\n",
    "    \n",
    "    # return the minimum average training error (rmse for interpretation)\n",
    "    best_train_error = np.min(mean_train_errors)\n",
    "    best_train_rmse = np.sqrt(best_train_error)\n",
    "    best_val_error = np.min(mean_val_errors)\n",
    "    best_val_rmse = np.sqrt(best_val_error)\n",
    "    return best_train_rmse, best_val_rmse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297a748-5bdd-4682-abf5-eae979502be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# loop through all proteins\n",
    "outputdir = \"/path/to/outputdir/\"\n",
    "for i in range(len(poi_list)):\n",
    "    logger.debug(f\"i index {i}\")\n",
    "    print(\"i index:\", i)\n",
    "    start_time = time.time()\n",
    "    poi = poi_list[i]\n",
    "    logger.debug(f\"Started {poi}\")\n",
    "    bootstrap = 10 \n",
    "    poi, X, y = data_org(poi, df_reprs_final, prot)\n",
    "\n",
    "    predictions, train_error_dict, valid_error_dict =  bootstrap_nn(poi, X, y, bootstrap, outputdir)\n",
    "    poi, pcorrelation, ppvalue, scorrelation, spvalue, y_avg_pred = plot_correlation(poi, y, predictions, outputdir)\n",
    "    best_train_rmse_error, best_val_rmse_error = plot_learning_curve(poi, train_error_dict, valid_error_dict, outputdir)\n",
    "\n",
    "    predvalues_df = populate_dataframe(predvalues_df, poi, y_avg_pred, y)\n",
    "    predvalues_df.to_csv(outputdir+'/predvalues.csv', index=False)\n",
    "\n",
    "    df.loc[i] = [poi, pcorrelation, ppvalue, scorrelation, spvalue, best_train_rmse_error, best_val_rmse_error, None]\n",
    "    df.to_csv(outputdir+'/output.csv', index=False)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Time spent on grid search: {elapsed_time:.2f} seconds\")\n",
    "    logger.debug(f\"Time spent protein: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e346b20-09f6-4123-9201-aa0828dc3c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct pvalue\n",
    "output = pd.read_csv(outputdir+'/output.csv')\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed62a152-1b51-4186-b3ee-8fcf52e6e2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust p-values using Benjamini-Hochberg\n",
    "def benjamini_hochberg(p_values, alpha=0.05):\n",
    "    m = len(p_values)\n",
    "    sorted_indices = p_values.argsort()\n",
    "    sorted_p_values = p_values[sorted_indices]\n",
    "\n",
    "    adjusted_p_values = sorted_p_values.copy()\n",
    "    for i in reversed(range(m)):\n",
    "        if i == m - 1:\n",
    "            adjusted_p_values[i] = min(sorted_p_values[i], 1)\n",
    "        else:\n",
    "            adjusted_p_values[i] = min(sorted_p_values[i] * m / (i + 1), adjusted_p_values[i + 1])\n",
    "\n",
    "    # reordering to the original order of p-values\n",
    "    back_order_indices = sorted_indices.argsort()\n",
    "    return adjusted_p_values[back_order_indices]\n",
    "\n",
    "output['bh_corrected_spearman_pvalue'] = benjamini_hochberg(output['spearman_pvalue'].values)\n",
    "output['bh_corrected_pearson_pvalue'] = benjamini_hochberg(output['pearson_pvalue'].values)\n",
    "output.to_csv(outputdir+'/output_adjusted.csv', index=False)\n",
    "output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586bdf2c-03d2-4d02-9049-fe48ae1c4e57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a307f02-30f5-4cfd-abd6-3e8aba741566",
   "metadata": {},
   "source": [
    "# Using trained model to generate protein values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85097c39-db31-447c-a5a6-5ab510148e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# poi_list (same as above, list of protein names that you are predicting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e781c9c0-9af1-46bb-afce-06fb6d24717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_org(protein_name, latentdf):\n",
    "\n",
    "    #protein of interest\n",
    "    poi = protein_name #change this to select different proteins\n",
    "    poiname = poi + '_protein'\n",
    "    print(poiname)\n",
    "    cols_remove = ['labeling_time']\n",
    "    # cols_remove = ['sample_ID', 'labeling_time'] ## for bypatient split\n",
    "    latentdf_clean = latentdf.drop(columns=cols_remove)\n",
    "    return poi, latentdf_clean\n",
    "\n",
    "\n",
    "\n",
    "# Define the model architecture to match what was used during training\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 32)  # Input to hidden layer\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(32, 1)  # Hidden to output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# Updated generate_predictions function with model architecture definition and loading\n",
    "def generate_predictions(poi, latentdf_clean):\n",
    "    # Directory containing the models for the given poi\n",
    "    model_dir = f\"/path/to/saved_models/models/{poi}_model_weights\"\n",
    "    \n",
    "    # Prepare data columns for prediction\n",
    "    feature_columns = [col for col in latentdf_clean.columns if col.startswith(\"data_\")]\n",
    "    X = latentdf_clean[feature_columns].values  # Extract features for model input\n",
    "    patient_ids = latentdf_clean['patient_ids']  # Keep track of patient IDs\n",
    "\n",
    "    # Initialize model with the input size equal to the number of feature columns\n",
    "    input_size = X.shape[1]\n",
    "    model = RegressionModel(input_size)\n",
    "    \n",
    "    # Collect predictions for each bootstrap model\n",
    "    all_predictions = []\n",
    "    for i in range(10):\n",
    "        print('bootstrap model number', i+1)\n",
    "        model_path = os.path.join(model_dir, f\"bootstrap_model_{i}.pt\")\n",
    "        \n",
    "        # Load model weights for this bootstrap iteration, using weights_only=True for security\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        \n",
    "        # Generate predictions for the current bootstrap model\n",
    "        with torch.no_grad():\n",
    "            inputs = torch.tensor(X, dtype=torch.float32)\n",
    "            outputs = model(inputs).numpy().flatten()\n",
    "            all_predictions.append(outputs)\n",
    "    \n",
    "    # Calculate the averaged predictions across all bootstrap models\n",
    "    avg_predictions = np.mean(all_predictions, axis=0)\n",
    "    \n",
    "    # Combine patient IDs with averaged predictions into a dataframe\n",
    "    predvalues = pd.DataFrame({'patient_ids': patient_ids, f\"{poi}_prediction\": avg_predictions})\n",
    "    \n",
    "    return predvalues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30cc786-bd77-40dc-869f-6e68bb60dcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ehr_rep = pd.read_csv('/path/to/ehr_motor_representations/ehr_representations_random.csv')\n",
    "ehr_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba6720-1bbf-4e13-8989-888e2bfe21b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe to store RABIT proteomics\n",
    "all_predictions_df = pd.DataFrame()\n",
    "i = 0\n",
    "\n",
    "# loop through all proteins\n",
    "for poi in proteinlist:\n",
    "    print('index:', i)\n",
    "    print(poi)\n",
    "    print('data cleaning')\n",
    "    poi, latentdf_clean = data_org(poi, ehr_rep)\n",
    "    \n",
    "    print('generating values')\n",
    "    predvalues = generate_predictions(poi, latentdf_clean)\n",
    "    \n",
    "    print('merging data')\n",
    "    if all_predictions_df.empty:\n",
    "        all_predictions_df = predvalues\n",
    "    else:\n",
    "        all_predictions_df = all_predictions_df.merge(predvalues, on=\"patient_ids\", how=\"left\")\n",
    "    i += 1\n",
    "\n",
    "all_predictions_df.to_csv('/path/to/outputdir', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d41a3a0-2afa-4920-aa18-0503ff815cce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6813e1a-63dd-4676-8a16-ce1b3ecd9640",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "davidpython_updated",
   "language": "python",
   "name": "davidpython_updated"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
